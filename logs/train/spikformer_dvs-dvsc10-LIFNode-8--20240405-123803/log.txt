2024-04-05 12:38:03,566 -                train: [    INFO] - Training with a single process on 1 GPUs.
2024-04-05 12:38:03,599 -                train: [    INFO] - [MODEL ARCH]
Spikformer(
  (patch_embed): SPS(
    (encoder): Encoder()
    (proj_conv): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (proj_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (proj_lif): MyNode(
      (act_fun): MyGrad()
    )
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (proj_conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (proj_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (proj_lif1): MyNode(
      (act_fun): MyGrad()
    )
    (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (proj_conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (proj_bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (proj_lif2): MyNode(
      (act_fun): MyGrad()
    )
    (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (proj_conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (proj_bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (proj_lif3): MyNode(
      (act_fun): MyGrad()
    )
    (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (rpe_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (rpe_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (rpe_lif): MyNode(
      (act_fun): MyGrad()
    )
  )
  (block): ModuleList(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): SSA(
        (encoder): Encoder()
        (q_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (q_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (q_lif): MyNode(
          (act_fun): MyGrad()
        )
        (k_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (k_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (k_lif): MyNode(
          (act_fun): MyGrad()
        )
        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (v_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (v_lif): MyNode(
          (act_fun): MyGrad()
        )
        (attn_drop): Dropout(p=0.2, inplace=False)
        (res_lif): MyNode(
          (act_fun): MyGrad()
        )
        (attn_lif): MyNode(
          (act_fun): MyGrad()
        )
        (proj_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (proj_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj_lif): MyNode(
          (act_fun): MyGrad()
        )
        (TIM): TIM(
          (encoder): Encoder()
          (interactor): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))
          (in_lif): MyNode(
            (act_fun): MyGrad()
          )
          (out_lif): MyNode(
            (act_fun): MyGrad()
          )
        )
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (encoder): Encoder()
        (fc1_conv): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))
        (fc1_bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (fc1_lif): MyNode(
          (act_fun): MyGrad()
        )
        (fc2_conv): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))
        (fc2_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (fc2_lif): MyNode(
          (act_fun): MyGrad()
        )
      )
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): SSA(
        (encoder): Encoder()
        (q_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (q_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (q_lif): MyNode(
          (act_fun): MyGrad()
        )
        (k_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (k_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (k_lif): MyNode(
          (act_fun): MyGrad()
        )
        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (v_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (v_lif): MyNode(
          (act_fun): MyGrad()
        )
        (attn_drop): Dropout(p=0.2, inplace=False)
        (res_lif): MyNode(
          (act_fun): MyGrad()
        )
        (attn_lif): MyNode(
          (act_fun): MyGrad()
        )
        (proj_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (proj_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj_lif): MyNode(
          (act_fun): MyGrad()
        )
        (TIM): TIM(
          (encoder): Encoder()
          (interactor): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))
          (in_lif): MyNode(
            (act_fun): MyGrad()
          )
          (out_lif): MyNode(
            (act_fun): MyGrad()
          )
        )
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (encoder): Encoder()
        (fc1_conv): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))
        (fc1_bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (fc1_lif): MyNode(
          (act_fun): MyGrad()
        )
        (fc2_conv): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))
        (fc2_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (fc2_lif): MyNode(
          (act_fun): MyGrad()
        )
      )
    )
  )
  (head): Linear(in_features=256, out_features=10, bias=True)
)
2024-04-05 12:38:03,600 -                train: [    INFO] - learning rate is 0.000078
2024-04-05 12:38:03,600 -                train: [    INFO] - Model spikformer_dvs created, param count: 2570844
2024-04-05 12:38:07,146 -                train: [    INFO] - [OPTIMIZER]
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 7.8125e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 7.8125e-05
    maximize: False
    weight_decay: 0.0001
)
2024-04-05 12:38:07,146 -                train: [    INFO] - AMP not enabled. Training in float32.
2024-04-05 12:38:07,147 -                train: [    INFO] - Scheduled epochs: 110
2024-04-05 12:38:08,081 -                train: [    INFO] - Train: 0 [   0/562 (  0%)]  Loss:  2.304942 (2.3049)  Acc@1: 12.5000 (12.5000)  Acc@5: 43.7500 (43.7500)  Time: 0.850s,   18.82/s  (0.850s,   18.82/s)  LR: 1.000e-06  Data: 0.586 (0.586)
2024-04-05 12:38:14,384 -                train: [    INFO] - Train: 0 [  50/562 (  9%)]  Loss:  2.339357 (2.3221)  Acc@1:  6.2500 ( 9.3750)  Acc@5: 31.2500 (37.5000)  Time: 0.125s,  128.12/s  (0.140s,  114.60/s)  LR: 1.000e-06  Data: 0.000 (0.012)
